# ğŸš€ Automated Data Pipeline - Complete Guide

Comprehensive documentation for the automated data ingestion, transformation, and monitoring system.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
- [Pipeline Stages](#pipeline-stages)
- [Monitoring](#monitoring)
- [Configuration](#configuration)
- [Workflow Examples](#workflow-examples)
- [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Overview

The automated data pipeline transforms raw Excel files into a structured SQLite database with validated, aggregated data ready for analysis. It eliminates manual data processing and ensures data quality through automated validation.

### Key Benefits
- âœ… **95% time savings** - No more manual Excel processing
- âœ… **Zero human error** - Automated validation catches issues
- âœ… **Real-time processing** - File watcher triggers instant ingestion
- âœ… **Audit trail** - Complete logging of all operations
- âœ… **Scheduling** - Run daily/weekly automatically

---

## âœ¨ Features

### ğŸ”„ Data Ingestion
- **Multi-file Support**: Process multiple Excel files automatically
- **Format Validation**: Ensures data meets required format
- **Error Handling**: Graceful handling of corrupt/missing files
- **Progress Tracking**: Real-time logging of ingestion progress

### ğŸ” Data Validation
- **Table Existence Checks**: Verifies required tables exist
- **Row Count Validation**: Ensures minimum data thresholds
- **Negative Value Detection**: Flags suspicious data
- **Data Type Verification**: Confirms correct column types

### ğŸ”„ Data Transformation
- **Vendor Aggregation**: Creates summary by vendor/product
- **KPI Calculation**: Auto-calculates profit, margins, turnover
- **Deduplication**: Removes duplicate records
- **Standardization**: Normalizes vendor names and descriptions

### ğŸ“¦ File Management
- **Automatic Archiving**: Moves processed files to archive
- **Timestamp Tracking**: Adds processing date to filenames
- **Folder Organization**: Keeps data directory clean

### â° Automation
- **File Watcher**: Auto-triggers on new file detection
- **Scheduled Execution**: Daily/weekly/custom schedules
- **Cooldown Control**: Prevents duplicate processing

---

## ğŸ“ Project Structure

```
vendor-analytics-project/
â”œâ”€â”€ pipeline/                    â† Automation modules
â”‚   â”œâ”€â”€ __init__.py             â† Package initializer
â”‚   â”œâ”€â”€ pipeline.py             â† Core data processing
â”‚   â”œâ”€â”€ watcher.py              â† File monitoring
â”‚   â”œâ”€â”€ config.py               â† Configuration
â”‚   â””â”€â”€ run.py                  â† Command interface
â”œâ”€â”€ data/                        â† Data storage
â”‚   â”œâ”€â”€ sales.xlsx              â† Source files (drop here)
â”‚   â”œâ”€â”€ purchases.xlsx          
â”‚   â””â”€â”€ archive/                â† Processed files (auto-moved)
â”‚       â”œâ”€â”€ sales_20240115.xlsx
â”‚       â””â”€â”€ purchases_20240115.xlsx
â”œâ”€â”€ logs/                        â† System logs
â”‚   â”œâ”€â”€ pipeline.log            â† Processing logs
â”‚   â”œâ”€â”€ watcher.log             â† File monitor logs
â”‚   â””â”€â”€ ingestion_db.log        â† Database logs
â”œâ”€â”€ inventory.db                 â† SQLite database
â””â”€â”€ requirements.txt             â† Dependencies
```

---

## ğŸ“¦ Installation

### Prerequisites
```bash
# Python 3.11+ required
python --version
```

### Install Dependencies
```bash
pip install pandas openpyxl sqlalchemy schedule watchdog
```

Or from requirements.txt:
```bash
pip install -r requirements.txt
```

### Verify Installation
```bash
python -c "import pandas, sqlalchemy, watchdog; print('âœ… All packages installed')"
```

---

## ğŸš€ Usage

### Option 1: Run Pipeline Once

Process all files in the data folder:

```bash
cd pipeline
python pipeline.py
```

**With file archiving:**
```bash
python pipeline.py --archive
```

**Validate data only (no processing):**
```bash
python pipeline.py --validate-only
```

---

### Option 2: Scheduled Pipeline

Run automatically every 24 hours:

```bash
python pipeline.py --schedule 24 --archive
```

**Custom schedule (every 6 hours):**
```bash
python pipeline.py --schedule 6 --archive
```

**What happens:**
- Runs immediately once
- Then runs every X hours
- Press `Ctrl+C` to stop
- Processes and archives files each time

---

### Option 3: File Watcher (Auto-trigger)

Automatically process new files as they arrive:

```bash
python watcher.py
```

**Custom cooldown:**
```bash
python watcher.py --cooldown 60
```

**What happens:**
- Watches `data/` folder continuously
- Triggers pipeline when .xlsx file added
- Respects cooldown period (default 30 seconds)
- Prevents duplicate processing

---

### Option 4: Interactive Menu

Easy-to-use menu interface:

```bash
python run.py
```

**Menu Options:**
```
1. ğŸš€ Run Pipeline Once (with archiving)
2. â° Start Scheduled Pipeline (every 24 hours)
3. ğŸ‘ï¸  Start File Watcher (auto-trigger on new files)
4-6. (Analytics options)
7-9. (Alert options)
10-11. (Dashboard options)
```

---

## ğŸ”„ Pipeline Stages

### Stage 1: Data Ingestion

**Purpose**: Load Excel files into database

**Process:**
1. Scan `data/` folder for .xlsx files
2. Read each file with pandas
3. Validate required columns exist
4. Insert into database (replace if exists)
5. Log results

**Output:**
- `sales` table - Raw sales data
- `purchases` table - Raw purchase data

**Logging:**
```
2024-01-15 10:30:00 - INFO - ğŸ“ Found 2 Excel files
2024-01-15 10:30:01 - INFO - ğŸ“¥ Processing: sales.xlsx
2024-01-15 10:30:15 - INFO - âœ… Ingested sales: 1048575 rows, 14 cols
2024-01-15 10:30:16 - INFO - ğŸ“¥ Processing: purchases.xlsx
2024-01-15 10:30:30 - INFO - âœ… Ingested purchases: 1048575 rows, 16 cols
2024-01-15 10:30:30 - INFO - âœ… Ingestion complete: 2/2 files
2024-01-15 10:30:30 - INFO - â±ï¸ Time taken: 0.50 minutes
```

---

### Stage 2: Data Validation

**Purpose**: Ensure data quality and integrity

**Checks Performed:**

1. **Table Existence**
   ```
   âœ… Table 'sales': 1048575 records
   âœ… Table 'purchases': 1048575 records
   ```

2. **Empty Table Detection**
   ```
   âš ï¸ Table 'sales' is empty
   ```

3. **Negative Value Detection**
   ```
   âš ï¸ Found 5 negative sales values
   ```

4. **Data Type Verification**
   - Ensures numeric columns are numeric
   - Checks date formats
   - Validates text fields

**Output:**
- Validation passes: Continue to next stage
- Validation fails: Log warnings, continue with caution
- Critical errors: Stop pipeline

---

### Stage 3: Data Transformation

**Purpose**: Create aggregated vendor summary

**SQL Query:**
```sql
CREATE TABLE vendor_sales_summary AS
SELECT 
    s.VendorName,
    s.Description,
    SUM(s.SalesQuantity) AS TotalSalesQuantity,
    SUM(s.SalesDollars) AS TotalSalesDollars,
    SUM(p.Quantity) AS TotalPurchaseQuantity,
    SUM(p.Dollars) AS TotalPurchaseDollars,
    (SUM(s.SalesDollars) - SUM(p.Dollars)) AS GrossProfit,
    CASE 
        WHEN SUM(s.SalesDollars) > 0 
        THEN ((SUM(s.SalesDollars) - SUM(p.Dollars)) / SUM(s.SalesDollars)) * 100
        ELSE 0 
    END AS ProfitMargin,
    CASE 
        WHEN AVG(p.Quantity) > 0 
        THEN SUM(s.SalesQuantity) / AVG(p.Quantity)
        ELSE 0 
    END AS StockTurnover,
    CASE 
        WHEN SUM(p.Dollars) > 0 
        THEN SUM(s.SalesDollars) / SUM(p.Dollars)
        ELSE 0 
    END AS SalesToPurchaseRatio
FROM sales s
LEFT JOIN purchases p 
    ON s.VendorName = p.VendorName 
    AND s.Description = p.Description
GROUP BY s.VendorName, s.Description
HAVING SUM(s.SalesDollars) > 0 
    AND SUM(p.Dollars) > 0
```

**Output:**
- `vendor_sales_summary` table with calculated KPIs
- Ready for analytics and dashboards

---

### Stage 4: File Archiving (Optional)

**Purpose**: Keep data folder organized

**Process:**
1. Find all .xlsx files in `data/`
2. Add timestamp to filename
3. Move to `data/archive/`
4. Log each move

**Example:**
```
Before:
data/sales.xlsx
data/purchases.xlsx

After:
data/archive/sales_20240115_103000.xlsx
data/archive/purchases_20240115_103000.xlsx
```

**Logging:**
```
2024-01-15 10:30:30 - INFO - ğŸ“¦ Step 4: Archiving processed files...
2024-01-15 10:30:30 - INFO - ğŸ“¦ Archived: sales.xlsx -> sales_20240115_103000.xlsx
2024-01-15 10:30:30 - INFO - ğŸ“¦ Archived: purchases.xlsx -> purchases_20240115_103000.xlsx
```

---

## ğŸ“Š Monitoring

### Check Logs

**Pipeline logs:**
```bash
# View last 50 lines
tail -n 50 logs/pipeline.log

# Follow in real-time
tail -f logs/pipeline.log

# Windows
type logs\pipeline.log
```

**Watcher logs:**
```bash
tail -f logs/watcher.log
```

### Log Levels

| Level | Symbol | Meaning |
|-------|--------|---------|
| INFO | âœ… | Normal operation |
| WARNING | âš ï¸ | Issue detected, continuing |
| ERROR | âŒ | Critical failure |

### Success Indicators

**Pipeline completed successfully:**
```
======================================================================
âœ… PIPELINE COMPLETED: 0.50 minutes
======================================================================
```

**Watcher running:**
```
======================================================================
ğŸ‘ï¸ Watching folder: /path/to/data
â±ï¸ Cooldown period: 30 seconds
ğŸ”„ Add .xlsx files to trigger the pipeline automatically
Press Ctrl+C to stop
======================================================================
```

---

## âš™ï¸ Configuration

### Edit config.py

```python
from pathlib import Path

# Paths
BASE_DIR = Path(__file__).parent.parent
DATA_FOLDER = BASE_DIR / 'data'
ARCHIVE_FOLDER = DATA_FOLDER / 'archive'
LOG_FOLDER = BASE_DIR / 'logs'
DB_PATH = BASE_DIR / 'inventory.db'

# Database
DATABASE_URL = f"sqlite:///{DB_PATH}"

# Pipeline Settings
PIPELINE_SCHEDULE_HOURS = 24      # Change schedule frequency
FILE_WATCH_COOLDOWN = 30          # Change cooldown period

# Data Validation
MIN_RECORDS_THRESHOLD = 10        # Minimum rows per table
ALLOWED_FILE_EXTENSIONS = ['.xlsx', '.xls']  # File types
```

### Common Customizations

**Change schedule interval:**
```python
PIPELINE_SCHEDULE_HOURS = 12  # Run every 12 hours
```

**Change watcher cooldown:**
```python
FILE_WATCH_COOLDOWN = 60  # Wait 60 seconds
```

**Add more file types:**
```python
ALLOWED_FILE_EXTENSIONS = ['.xlsx', '.xls', '.csv']
```

---

## ğŸ’¼ Workflow Examples

### Daily Automated Updates

**Setup:**
```bash
# Start file watcher in background
cd pipeline
python watcher.py &
```

**Usage:**
1. Drop new Excel files into `data/` folder
2. Pipeline runs automatically
3. Files moved to archive
4. Database updated
5. Ready for dashboards

---

### Weekly Scheduled Updates

**Windows Task Scheduler:**
1. Open Task Scheduler
2. Create Basic Task
3. Trigger: Weekly, Sunday, 12:00 AM
4. Action: Start program
   - Program: `python`
   - Arguments: `C:\path\to\pipeline\pipeline.py --archive`
   - Start in: `C:\path\to\project`

**Linux/Mac Cron:**
```bash
# Edit crontab
crontab -e

# Add line (runs Sundays at midnight)
0 0 * * 0 cd /path/to/project && python pipeline/pipeline.py --archive
```

---

### Manual Processing

**When to use:**
- First-time setup
- Testing changes
- One-off data loads

**Steps:**
```bash
# 1. Place files in data/ folder
cp /path/to/new/sales.xlsx data/
cp /path/to/new/purchases.xlsx data/

# 2. Run pipeline
cd pipeline
python pipeline.py --archive

# 3. Check logs
tail logs/pipeline.log

# 4. Verify database
python -c "
from sqlalchemy import create_engine
import pandas as pd
engine = create_engine('sqlite:///inventory.db')
df = pd.read_sql('SELECT COUNT(*) as count FROM vendor_sales_summary', engine)
print(f'Vendors in database: {df.iloc[0][0]}')
"
```

---

## ğŸ› Troubleshooting

### Issue: "No Excel files found"

**Symptoms:**
```
âš ï¸ No Excel files found in data folder
```

**Solution:**
1. Check files are in correct location:
   ```bash
   ls data/
   ```
2. Ensure files end with `.xlsx` or `.xls`
3. Check file permissions (read access)

---

### Issue: "Database connection failed"

**Symptoms:**
```
âŒ Database connection failed: unable to open database file
```

**Solutions:**

**1. Database is locked:**
```bash
# Close any programs using the database
# Delete lock file
rm inventory.db-journal

# Restart pipeline
python pipeline.py
```

**2. Permissions issue:**
```bash
# Check file permissions
ls -l inventory.db

# Fix permissions
chmod 644 inventory.db
```

**3. Corrupted database:**
```bash
# Backup and recreate
mv inventory.db inventory.db.backup
python pipeline.py --archive
```

---

### Issue: "Validation failed"

**Symptoms:**
```
âš ï¸ Table 'sales' is empty
âŒ Pipeline failed: Data loading error
```

**Solutions:**

**1. Check Excel file format:**
```python
import pandas as pd
df = pd.read_excel('data/sales.xlsx')
print(df.columns.tolist())  # Check column names
print(df.head())             # Check data
```

**2. Verify required columns:**

**sales.xlsx needs:**
- VendorName
- Description
- SalesQuantity
- SalesDollars

**purchases.xlsx needs:**
- VendorName
- Description
- Quantity
- Dollars

**3. Check for empty files:**
```bash
# Check file size
ls -lh data/sales.xlsx
# Should be > 0 bytes
```

---

### Issue: "Pipeline runs but dashboard shows no data"

**Symptoms:**
- Pipeline completes successfully
- Dashboard shows "No data available"

**Solutions:**

**1. Check if vendor_sales_summary exists:**
```python
from sqlalchemy import create_engine
import pandas as pd

engine = create_engine('sqlite:///inventory.db')
tables = pd.read_sql("SELECT name FROM sqlite_master WHERE type='table'", engine)
print(tables)  # Should show vendor_sales_summary
```

**2. Check row count:**
```python
df = pd.read_sql("SELECT COUNT(*) as count FROM vendor_sales_summary", engine)
print(df)  # Should be > 0
```

**3. Verify data filters:**
- Dashboard filters out records with zero profit
- Check if all your data has valid profit values

**4. Clear dashboard cache:**
```bash
# In dashboard, press 'C' key or click menu â†’ Clear cache
```

---

### Issue: Watcher triggers multiple times

**Symptoms:**
```
ğŸ”„ Pipeline triggered
â³ Cooldown active, skipping trigger
ğŸ”„ Pipeline triggered
```

**Solution:**

**Increase cooldown:**
```bash
python watcher.py --cooldown 60
```

Or edit `config.py`:
```python
FILE_WATCH_COOLDOWN = 60
```

---

### Issue: "Processing takes too long"

**Symptoms:**
- Pipeline runs for 10+ minutes
- Large Excel files (1M+ rows)

**Solutions:**

**1. Normal for large datasets:**
- 1M rows â‰ˆ 5-7 minutes (expected)

**2. Optimize if needed:**
- Split large files into smaller chunks
- Use CSV instead of XLSX (faster)
- Run during off-peak hours

**3. Monitor progress:**
```bash
# Watch logs in real-time
tail -f logs/pipeline.log
```

---

## ğŸ“ˆ Best Practices

### 1. Backup Before Major Changes
```bash
# Backup database
cp inventory.db inventory.db.backup

# Backup data folder
cp -r data data.backup
```

### 2. Test with Sample Data First
```bash
# Create test folder
mkdir data_test
cp sample_data.xlsx data_test/

# Test pipeline
python pipeline.py  # without --archive
```

### 3. Monitor Logs Regularly
```bash
# Check for warnings/errors
grep "ERROR\|WARNING" logs/pipeline.log
```

### 4. Archive Processed Files
```bash
# Always use --archive flag
python pipeline.py --archive
```

### 5. Schedule During Off-Peak Hours
- Run large data loads at night
- Avoid during business hours

---

## ğŸ¯ Performance Tips

### For Large Datasets (1M+ rows):

**1. Use chunking:**
```python
# In pipeline.py, modify load:
df = pd.read_excel(file_path, chunksize=100000)
```

**2. Optimize database:**
```python
# Add indexes
from sqlalchemy import text
engine.execute(text("CREATE INDEX idx_vendor ON sales(VendorName)"))
```

**3. Increase memory:**
```python
# If out of memory errors
import pandas as pd
pd.set_option('mode.chained_assignment', None)
```

---

## ğŸ“ Summary

**Pipeline Capabilities:**
- âœ… Automated data ingestion from Excel
- âœ… Real-time file monitoring
- âœ… Scheduled execution
- âœ… Data validation and quality checks
- âœ… KPI calculation and aggregation
- âœ… File archiving and organization
- âœ… Comprehensive logging

**Typical Performance:**
- 100K rows: ~30 seconds
- 500K rows: ~2 minutes
- 1M+ rows: ~5-7 minutes

**Recommended Workflow:**
1. Drop files in `data/` folder
2. Run pipeline (manual or automatic)
3. Check logs for success
4. View results in dashboard
5. Repeat as needed

---

## ğŸ†˜ Getting Help

If issues persist:

1. **Check logs**: `logs/pipeline.log`
2. **Review error messages**: Read carefully
3. **Search GitHub issues**: See if others had same problem
4. **Open new issue**: Include error message, steps, Python version

---

**Ready to automate your data processing!** ğŸš€
